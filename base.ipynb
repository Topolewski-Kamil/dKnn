{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Topolewski-Kamil/dKnn/blob/main/base.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Deep k-Nearest Neighbours**"
      ],
      "metadata": {
        "id": "845beWVYyBE1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bimNvVLP0XD"
      },
      "source": [
        "Mount drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLe7HBrK7CbQ",
        "outputId": "208943dc-5f8e-4586-e96c-530b31cab7ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive/disseration\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/My Drive/disseration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJn05KsnfDLI"
      },
      "source": [
        "Import modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SnZbGpex18E5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4be8d0f0-1950-469c-be42-ebdaf27beb29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pynndescent in /usr/local/lib/python3.7/dist-packages (0.5.6)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from pynndescent) (1.1.0)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.7/dist-packages (from pynndescent) (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from pynndescent) (1.4.1)\n",
            "Requirement already satisfied: llvmlite>=0.30 in /usr/local/lib/python3.7/dist-packages (from pynndescent) (0.34.0)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.7/dist-packages (from pynndescent) (0.51.2)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from numba>=0.51.2->pynndescent) (1.21.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.51.2->pynndescent) (57.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->pynndescent) (3.1.0)\n",
            "Requirement already satisfied: falconn in /usr/local/lib/python3.7/dist-packages (1.3.1)\n",
            "Requirement already satisfied: annoy in /usr/local/lib/python3.7/dist-packages (1.17.0)\n",
            "Requirement already satisfied: faiss-gpu in /usr/local/lib/python3.7/dist-packages (1.7.2)\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "## version for plotting bar labels (unstable)\n",
        "# !pip install matplotlib --upgrade \n",
        "\n",
        "## version for the rest of plots (stable)\n",
        "# !pip install matplotlib==3.1.3 \n",
        "\n",
        "# Standard python libraries\n",
        "import numpy as np\n",
        "import string\n",
        "import copy\n",
        "from bisect import bisect_left\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# tensorflow libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras import backend as K\n",
        "from keras.models import load_model, Sequential\n",
        "from keras.losses import MSE\n",
        "from keras.layers import Conv2D, Dense, Dropout, Flatten, MaxPooling2D, BatchNormalization\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "# approximate nearest neighbours libraries\n",
        "!pip install pynndescent\n",
        "!pip install falconn\n",
        "!pip install annoy\n",
        "!pip install faiss-gpu\n",
        "import faiss\n",
        "import pynndescent\n",
        "import falconn\n",
        "import annoy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_pickle(save_object, filename):\n",
        "  with open(filename, 'wb') as f:\n",
        "    pickle.dump(save_object, f)\n",
        "\n",
        "def load_pickle(filename):\n",
        "  with open(filename, 'rb') as f:\n",
        "    loaded_object = pickle.load(f)\n",
        "    return loaded_object"
      ],
      "metadata": {
        "id": "k6FvfQiGxqkA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvbVxy5tfCwQ"
      },
      "source": [
        "Import and preprocess MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hXAsY2jo1_y7"
      },
      "outputs": [],
      "source": [
        "# number of classes\n",
        "num_classes = 10 \n",
        "\n",
        "# input shape of images fed into CNN\n",
        "input_shape = (28, 28, 1)\n",
        "\n",
        "# import mnist data\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Scale to [0, 1] size\n",
        "x_train = x_train.astype(\"float32\") / 255\n",
        "x_test = x_test.astype(\"float32\") / 255\n",
        "\n",
        "# Make images shape (28, 28, 1)\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "\n",
        "# keep raw labels in separate array\n",
        "y_test_raw = y_test[750:10000]\n",
        "y_train_raw = y_train\n",
        "y_cal_raw = y_test[0:750]\n",
        "\n",
        "# convert labels to one hot enconding\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "# split test set into calibration set\n",
        "x_cal = x_test[0:750] \n",
        "y_cal = y_test[0:750]\n",
        "x_test = x_test[750:10000]\n",
        "y_test = y_test[750:10000]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model 1** - recreation of N. Papernot and P. McDaniel model "
      ],
      "metadata": {
        "id": "Ysyk2wys5Glv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNU51mw7fU3A"
      },
      "source": [
        "Initialize CNN model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model architechture\n",
        "model1 = Sequential(\n",
        "    [\n",
        "        keras.Input(shape=input_shape),\n",
        "        Conv2D(64, kernel_size=(8, 8), strides=(2,2), padding='same', activation='relu', input_shape=x_train.shape[1:]),\n",
        "        Conv2D(128, kernel_size=(6, 6), strides=(2,2), padding='valid', activation='relu'),\n",
        "        Conv2D(128, kernel_size=(5, 5), strides=(1,1), padding='valid', activation='relu'),\n",
        "        Flatten(),\n",
        "        Dense(10, activation=tf.nn.softmax),\n",
        "    ]\n",
        ")\n",
        "\n",
        "batch_size = 500\n",
        "epochs = 8\n",
        "\n",
        "adam_opt = keras.optimizers.Adam(learning_rate=0.001)\n",
        "model1.compile(loss=\"categorical_crossentropy\", optimizer=adam_opt, metrics=[\"accuracy\"])\n",
        "\n",
        "# indexes of layers that will be indexed and used for querying nearest neibouts\n",
        "neighbours_layers_indexes = [0,1,2]\n",
        "knn_layers_count = len(neighbours_layers_indexes)\n",
        "\n",
        "# number of neigbours used for Aproximate Nearest Neighbour Algorithm\n",
        "K_NEIGHBOURS = 75"
      ],
      "metadata": {
        "id": "IrTFfvZfd8t1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train and evaluate model"
      ],
      "metadata": {
        "id": "z12frsoO9yZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# try loading model from files\n",
        "try:\n",
        "  model1 = load_model('model1_three_lay/model.h5')\n",
        "  print(\"loaded model from disc\")\n",
        "\n",
        "# otherwise train from scratch\n",
        "except OSError:\n",
        "  print(\"failed to load model from disc\")\n",
        "  model1.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
        "  # model.save('deepKNN_model_new.h5')  # creates a HDF5 file\n",
        "    \n",
        "score = model1.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test accuracy:\", score[1])"
      ],
      "metadata": {
        "id": "IO3fDipN9yIs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69c887c1-23e7-42f0-ca9c-a9bb5e959a8d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded model from disc\n",
            "Test accuracy: 0.9911351203918457\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model 2** - my *Model*"
      ],
      "metadata": {
        "id": "h-vRkjY913XP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2jKKspx13XW"
      },
      "source": [
        "Initialize CNN model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model architechture\n",
        "model2 = Sequential(\n",
        "    [\n",
        "        keras.Input(shape=input_shape),\n",
        "        Conv2D(filters=64, kernel_size = (3,3), activation=\"relu\", input_shape=(28,28,1)),\n",
        "        Conv2D(filters=64, kernel_size = (3,3), activation=\"relu\"),\n",
        "        MaxPooling2D(pool_size=(2,2)),\n",
        "        BatchNormalization(),\n",
        "     \n",
        "        Conv2D(filters=128, kernel_size = (3,3), activation=\"relu\"),\n",
        "        Conv2D(filters=128, kernel_size = (3,3), activation=\"relu\"),\n",
        "        MaxPooling2D(pool_size=(2,2)),\n",
        "        BatchNormalization(),   \n",
        "\n",
        "        Conv2D(filters=256, kernel_size = (3,3), activation=\"relu\"),\n",
        "        MaxPooling2D(pool_size=(2,2)),\n",
        "        BatchNormalization(),\n",
        "            \n",
        "        Flatten(),\n",
        "        Dense(512,activation=\"relu\"),\n",
        "        Dense(10,activation=\"softmax\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "batch_size2 = 500\n",
        "epochs2 = 8\n",
        "\n",
        "adam_opt = keras.optimizers.Adam(learning_rate=0.001)\n",
        "model1.compile(loss=\"categorical_crossentropy\", optimizer=adam_opt, metrics=[\"accuracy\"])\n",
        "\n",
        "# indexes of layers that will be indexed and used for querying nearest neibouts\n",
        "neighbours_layers_indexes2 = [2,6,9]\n",
        "knn_layers_count2 = len(neighbours_layers_indexes2)\n",
        "\n",
        "# number of neigbours used for Aproximate Nearest Neighbour Algorithm\n",
        "K_NEIGHBOURS = 75"
      ],
      "metadata": {
        "id": "TdmTPLYn7Qa5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train and evaluate model"
      ],
      "metadata": {
        "id": "0n2jNtnP3fKp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# try loading model from files\n",
        "try:\n",
        "  model2 = load_model('model2/experiment2/deepKNN_model9947.h5')\n",
        "  print(\"loaded model from disc\")\n",
        "\n",
        "# otherwise train from scratch\n",
        "except OSError:\n",
        "  print(\"failed to load model from disc\")\n",
        "  model2.fit(x_train, y_train, batch_size=batch_size2, epochs=epochs2, validation_split=0.1)\n",
        "  # model2.save('model2/deepKNN_model2.h5')  # creates a HDF5 file\n",
        "  print(\"saved model successfully\")\n",
        "\n",
        "score = model2.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Test accuracy:\", score[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Adlc1djg2Stp",
        "outputId": "b17b190f-4c25-4284-99f4-a4cd4720520a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded model from disc\n",
            "Test loss: 0.02564595453441143\n",
            "Test accuracy: 0.9947026968002319\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Activations"
      ],
      "metadata": {
        "id": "ITTFlCjbndjE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get activations for each dataset"
      ],
      "metadata": {
        "id": "koERwqrrnVRT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def layers_activations_small(model, dataset, neighbours_layers_indexes):\n",
        "    reshaped_output_layer = []\n",
        "    for i in neighbours_layers_indexes: # 0,1,2-convolutions layer, 5-dense layer\n",
        "        layer = model.layers[i]\n",
        "        lay_act = K.function([model.layers[0].input], [layer.output])([dataset])[0]\n",
        "        if i != 5:\n",
        "          reshaped_output_layer.append(lay_act.reshape(lay_act.shape[0], lay_act.shape[1]*lay_act.shape[2]*lay_act.shape[3]))\n",
        "        else: \n",
        "          reshaped_output_layer.append(lay_act)\n",
        "    \n",
        "    for layer in reshaped_output_layer:\n",
        "        layer /= np.linalg.norm(layer, axis=1).reshape(-1, 1)\n",
        "\n",
        "    return reshaped_output_layer\n",
        "\n",
        "def layers_activations_big_data(model, dataset, neighbours_layers_indexes):\n",
        "    reshaped_output_layer = []\n",
        "\n",
        "    for i in neighbours_layers_indexes: # 0,1,2-convolutions layer, 5-dense layer\n",
        "        layer = model.layers[i]\n",
        "        lay_act_arr = []\n",
        "        chunk_size = 10000\n",
        "        for i in range(0, 10000, chunk_size):\n",
        "          lay_act = K.function([model.layers[0].input], [layer.output])([dataset[i:i+chunk_size]])[0]\n",
        "          lay_act_arr = np.array(lay_act)\n",
        "        for i in range(10000, len(dataset), chunk_size):\n",
        "          lay_act = K.function([model.layers[0].input], [layer.output])([dataset[i:i+chunk_size]])[0]\n",
        "          lay_act_arr = np.append(lay_act_arr, lay_act, axis=0)\n",
        "\n",
        "        reshaped_output_layer.append(lay_act_arr.reshape(lay_act_arr.shape[0], lay_act_arr.shape[1]*lay_act_arr.shape[2]*lay_act_arr.shape[3]))\n",
        "        # reshaped_output_layer.append(lay_act_arr)\n",
        "    \n",
        "    for layer in reshaped_output_layer:\n",
        "        layer /= np.linalg.norm(layer, axis=1).reshape(-1, 1)\n",
        "\n",
        "    return reshaped_output_layer\n",
        "\n",
        "def datasets_activations(model, model_adv, layers_activations, neighbours_layers_indexes):\n",
        "  activations_train = layers_activations(model, x_train, neighbours_layers_indexes)\n",
        "\n",
        "  activations_test = {}\n",
        "  for eps in model_adv.epsilons:\n",
        "    activations_test[eps] = layers_activations(model, model_adv.fgsm_adversaries[eps], neighbours_layers_indexes)\n",
        "\n",
        "  activations_calib = layers_activations(model, x_cal, neighbours_layers_indexes)\n",
        "\n",
        "  return [activations_train, activations_test, activations_calib]"
      ],
      "metadata": {
        "id": "j4gdg2ey2Z8u"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adversarial Examples"
      ],
      "metadata": {
        "id": "WH6LaitFAB9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## This class provides functionality for generating adversairial examples\n",
        "class Adversaries:\n",
        "  def __init__(self, model, epsilons, folder):\n",
        "    self.model = model\n",
        "    self.epsilons = copy.deepcopy(epsilons)\n",
        "    self.folder = folder\n",
        "\n",
        "    self.fgsm_adversaries = self.get_adversaries()\n",
        "    self.plot_accuracy_vs_epsilon()\n",
        "\n",
        "  def load_adversaries(self, folder):\n",
        "    adversaries = {}\n",
        "    for eps in self.epsilons:\n",
        "      eps_str = str(eps)\n",
        "      eps_str = eps_str.translate(str.maketrans('', '', string.punctuation))\n",
        "      path = folder + 'adv_datasets/adversaries' + eps_str  + '.npy'\n",
        "      adversaries[eps] = np.load(path)\n",
        "    return adversaries\n",
        "\n",
        "  # function saving adversarials examples\n",
        "  def save_adversaries(self, adversaries, folder):\n",
        "    for eps in self.epsilons:\n",
        "      eps_str = str(eps)\n",
        "      eps_str = eps_str.translate(str.maketrans('', '', string.punctuation))\n",
        "      filename = folder + 'adv_datasets/adversaries' + eps_str + '.npy'\n",
        "      np.save(filename, adversaries[eps])\n",
        "      print('saved ' + str(eps))\n",
        "\n",
        "  def generate_single_adversary(self, image, label, eps):\n",
        "\n",
        "    # Make images shape (1, 28, 28, 1)\n",
        "    image = tf.cast(image.reshape(1, 28, 28, 1), tf.float32)\n",
        "\n",
        "    # record our gradients\n",
        "    with tf.GradientTape() as tape:\n",
        "\n",
        "      # start tracing image by the Tape\n",
        "      tape.watch(image)\n",
        "\n",
        "      #compute prediction\n",
        "      pred = self.model(image)\n",
        "\n",
        "      # compute loss\n",
        "      loss = MSE(label, pred)\n",
        "\n",
        "      # calculate the gradient of loss function \n",
        "      grad = tape.gradient(loss, image)\n",
        "\n",
        "      # compute the sign of the gradient\n",
        "      sign = tf.sign(grad)\n",
        "\n",
        "      # create perturbation\n",
        "      perturbation = sign * eps\n",
        "\n",
        "      # apply perturbation to image\n",
        "      adversary = (image + perturbation).numpy()\n",
        "\n",
        "      # Make images shape (28, 28, 1)\n",
        "      adversary = adversary.reshape(28, 28, 1)\n",
        "      \n",
        "      return adversary\n",
        "\n",
        "  def get_adversaries(self):\n",
        "    try:\n",
        "      adversaries = self.load_adversaries(self.folder)\n",
        "      self.epsilons.insert(0, 0.0)\n",
        "      adversaries[0.0] = x_test\n",
        "      print('loaded adversarial datasets from disc')\n",
        "    except FileNotFoundError:\n",
        "      print('failed to load adversarial datasets from disc')\n",
        "      adversaries = {}\n",
        "      for eps in self.epsilons:\n",
        "        adv = []\n",
        "        for i in range(x_test.shape[0]):\n",
        "          adv.append(self.generate_single_adversary(x_test[i], y_test[i], eps))\n",
        "        adversaries[eps] = np.array(adv)\n",
        "        # self.save_adversaries(adversaries, folder)\n",
        "      self.epsilons.insert(0, 0.0)\n",
        "      adversaries[0.0] = x_test\n",
        "    return adversaries\n",
        "\n",
        "  def plot_accuracy_vs_epsilon(self):\n",
        "    accuracies_plot = []\n",
        "    for eps in self.epsilons:\n",
        "      adversary_score = self.model.evaluate(self.fgsm_adversaries[eps], y_test, verbose=0)\n",
        "      accuracies_plot.append(adversary_score[1] * 100)\n",
        "      print(adversary_score)\n",
        "\n",
        "    plt.figure(figsize=(5,5))\n",
        "    plt.plot(self.epsilons, accuracies_plot, linestyle='--', marker='o', color='b', label = 'DNN')\n",
        "\n",
        "    plt.yticks(np.arange(0, 101, step=10))\n",
        "    plt.xticks(np.arange(0, 0.16, step=0.025))\n",
        "    plt.xlabel(\"Epsilon\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.title(\"Accuracy vs Epsilon\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "  def plot_adv_examples(self):\n",
        "    counter = 0\n",
        "    plt.figure(figsize=(10,15))\n",
        "\n",
        "    for eps in self.epsilons:\n",
        "      adv_img = self.fgsm_adversaries[eps][10:15]\n",
        "      true_label = y_test[10:15]\n",
        "\n",
        "      adv_results = model1.predict(self.fgsm_adversaries[eps][10:15])\n",
        "      predicted_labels = []\n",
        "      for result in adv_results:\n",
        "        predicted_labels.append(np.argmax(result))\n",
        "\n",
        "      for i in range(5):\n",
        "          counter += 1\n",
        "          plt.subplot(len(self.epsilons), 5, counter)\n",
        "          plt.xticks([], [])\n",
        "          plt.yticks([], [])\n",
        "\n",
        "          if i == 0:\n",
        "            plt.ylabel('Eps: '+ str(eps), fontsize=14)\n",
        "\n",
        "          label = np.argmax(true_label[i])\n",
        "          if predicted_labels[i] != label:\n",
        "            incorrect_label = plt.title('Pred. label: ' + str(predicted_labels[i]))\n",
        "            plt.setp(incorrect_label, color='r')\n",
        "          else:\n",
        "            if eps == 0.0:\n",
        "              plt.title(r\"$\\bf{\" + 'True label:' + str(label) + \"}$\" + '\\n\\nPred. label:' + str(predicted_labels[i]))\n",
        "            else:\n",
        "              plt.title('Pred. label: ' + str(predicted_labels[i]))\n",
        "\n",
        "          plt.imshow(np.squeeze(adv_img[i]), cmap=\"gray\")\n",
        "    plt.savefig('adv_examples.png')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "zxdLmDW-AM-I"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyNND"
      ],
      "metadata": {
        "id": "hbfHUacwXai7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PyNND:\n",
        "  def __init__(self, model, activations_ttc, neighbours_layers_indexes, epsilons, folder):\n",
        "    self.model = model\n",
        "    self.neighbours_layers_indexes = neighbours_layers_indexes\n",
        "    self.activations_train = activations_ttc[0]\n",
        "    self.activations_test = activations_ttc[1]\n",
        "    self.activations_calib = activations_ttc[2]\n",
        "    self.knn_layers_count = len(neighbours_layers_indexes)\n",
        "    self.epsilons = epsilons\n",
        "    self.folder = folder\n",
        "    self.knn_eps = 0.1\n",
        "\n",
        "    index_act = self.index_layer()\n",
        "    self.neighbours = self.find_neighbours_test(index_act)\n",
        "    self.neighbours_calib = self.find_neighbours_calib(index_act)\n",
        "\n",
        "  def index_layer(self):\n",
        "    try: \n",
        "        index_act = load_pickle(self.folder + 'pynnd/neighbours_index.pkl')\n",
        "        print('loaded neighbours index from disc')\n",
        "    except FileNotFoundError:\n",
        "        print('failed to load neighbours index from disc')\n",
        "        index_act = {}\n",
        "        for layer in range(self.knn_layers_count):\n",
        "          index_act[layer] = pynndescent.NNDescent(self.activations_train[layer]) # index training data\n",
        "          index_act[layer].prepare() # prepare for faster query\n",
        "        # save_pickle(index_act, 'model1exact/neighbours_index.pkl')\n",
        "    return index_act\n",
        "\n",
        "  def query_index(self, index, activations):\n",
        "    neighbours = index.query(activations, k=K_NEIGHBOURS, epsilon=self.knn_eps)\n",
        "    neighbours_labels = y_train_raw[neighbours[0]]\n",
        "    return neighbours_labels\n",
        "\n",
        "  def find_neighbours_test(self, index):\n",
        "    try:\n",
        "      neighbours = load_pickle(self.folder + 'pynnd/neighbours.pkl')\n",
        "      print('loaded neighbours from disc')\n",
        "    except FileNotFoundError:\n",
        "      print('failed to load neighbours from disc')\n",
        "      neighbours = {}\n",
        "      for layer in range(knn_layers_count):\n",
        "        neighbours[layer] = {}\n",
        "        for eps in self.epsilons:\n",
        "          neighbours[layer][eps] = self.query_index(index[layer], self.activations_test[eps][layer])\n",
        "      # save_pickle(neighbors, 'model1exact/neighbours.pkl')\n",
        "    return neighbours\n",
        "\n",
        "  def find_neighbours_calib(self, index):\n",
        "    neigbours_calib = {}\n",
        "    for layer in range(self.knn_layers_count):\n",
        "      neigbours_calib[layer] = self.query_index(index[layer], self.activations_calib[layer])\n",
        "    return neigbours_calib"
      ],
      "metadata": {
        "id": "SJlI8FAhzrVq"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Annoy"
      ],
      "metadata": {
        "id": "EkK5h6A8XezZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Annoy:\n",
        "  def __init__(self, model, activations_ttc, neighbours_layers_indexes, epsilons, folder):\n",
        "    self.model = model\n",
        "    self.neighbours_layers_indexes = neighbours_layers_indexes\n",
        "    self.activations_train = activations_ttc[0]\n",
        "    self.activations_test = activations_ttc[1]\n",
        "    self.activations_calib = activations_ttc[2]\n",
        "    self.epsilons = epsilons\n",
        "    self.folder = folder\n",
        "\n",
        "    self.knn_layers_count = len(neighbours_layers_indexes)\n",
        "\n",
        "    index_act = self.index_layer()\n",
        "    self.neighbours = self.find_neighbours_test(index_act)\n",
        "    self.neighbours_calib = self.find_neighbours_calib(index_act)\n",
        "\n",
        "  def save_annoy_index(self, neighbours):\n",
        "    for layer in range(self.knn_layers_count):\n",
        "      neighbours[layer].save(self.folder + 'annoy/index/annoy_index_' + str(layer) + '.ann')\n",
        "\n",
        "  def load_annoy_index(self):\n",
        "    index_act = {}\n",
        "    for layer in range(self.knn_layers_count):\n",
        "      index_act[layer] = annoy.AnnoyIndex(self.activations_train[layer].shape[1], 'angular')\n",
        "      index_act[layer].load(self.folder +'annoy/index/annoy_index_' + str(layer) + '.ann')\n",
        "    return index_act\n",
        "\n",
        "  def load_annoy_neighbours(self):\n",
        "    neighbors_annoy = {}\n",
        "    for layer in range(knn_layers_count):\n",
        "      neighbors_annoy[layer] = load_pickle(self.folder + 'annoy/neighbours/neighbours_annoy_layer_' + str(layer + 1) + '.pkl')\n",
        "      for eps in self.epsilons:\n",
        "        neighbors_annoy[layer][eps] = y_train_raw[neighbors_annoy[layer][eps]]\n",
        "    return neighbors_annoy\n",
        "  \n",
        "  def index_layer(self):\n",
        "    try: \n",
        "      index_act = self.load_annoy_index()\n",
        "      print('loaded annoy index successfully')\n",
        "    except OSError:\n",
        "      print('failed to load neighbours index from disc')\n",
        "      index_act = {}\n",
        "      for layer in range(self.knn_layers_count):\n",
        "        index_act[layer] = annoy.AnnoyIndex(self.activations_train[layer].shape[1], 'angular')\n",
        "\n",
        "      for layer in range(self.knn_layers_count):\n",
        "        for i in range(self.activations_train[layer].shape[0]):\n",
        "          index_act[layer].add_item(i, self.activations_train[layer][i])\n",
        "        index_act[layer].build(50) # build 50 trees\n",
        "\n",
        "      # self.save_annoy_index(index_act)\n",
        "    return index_act\n",
        "\n",
        "  def find_neighbours_test(self, index):\n",
        "    try:\n",
        "      neighbours = self.load_annoy_neighbours()\n",
        "      print('loaded annoy neighbours')\n",
        "    except OSError:\n",
        "      print('failed to loaded annoy neighbours')\n",
        "      neighbours = {}\n",
        "      for layer in range(self.knn_layers_count):\n",
        "        neighbours[layer] = {}\n",
        "        for eps in self.epsilons:\n",
        "          arr = []\n",
        "          for act in self.activations_test[eps][layer]:\n",
        "            indexes = index[layer].get_nns_by_vector(act, K_NEIGHBOURS)\n",
        "            arr.append(y_train_raw[indexes])\n",
        "          neighbours[layer][eps] = np.array(arr)\n",
        "    return neighbours\n",
        "\n",
        "  def find_neighbours_calib(self, index):\n",
        "    neigbours_cal = {}\n",
        "    for layer in range(self.knn_layers_count):\n",
        "      arr = []\n",
        "      for act in self.activations_calib[layer]:\n",
        "        calib_indexes = index[layer].get_nns_by_vector(act, K_NEIGHBOURS)\n",
        "        arr.append(y_train_raw[calib_indexes])\n",
        "      neigbours_cal[layer] = np.array(arr)\n",
        "    return neigbours_cal"
      ],
      "metadata": {
        "id": "chyfDRWNXiQS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Falconn"
      ],
      "metadata": {
        "id": "sauhy9NhkdfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Falconn:\n",
        "  def __init__(self, model, activations_ttc, neighbours_layers_indexes, epsilons, folder):\n",
        "    self.model = model\n",
        "    self.neighbours_layers_indexes = neighbours_layers_indexes\n",
        "    self.activations_train = activations_ttc[0]\n",
        "    self.activations_test = activations_ttc[1]\n",
        "    self.activations_calib = activations_ttc[2]\n",
        "    self.epsilons = epsilons\n",
        "    self.folder = folder\n",
        "    self.knn_layers_count = len(neighbours_layers_indexes)\n",
        "    self.number_of_tables = 50\n",
        "\n",
        "    index_act, center_arr = self.index_layers()\n",
        "    print('finished indexing')\n",
        "\n",
        "    self.neighbours = self.find_neighbours_test(index_act, center_arr)\n",
        "    self.neighbours_calib = self.find_neighbours_calib(index_act)\n",
        "\n",
        "  def index_single_layer(self, act, center):\n",
        "    # act -= center\n",
        "    params_cp = falconn.LSHConstructionParameters()\n",
        "    params_cp.dimension = len(act[0])\n",
        "    params_cp.lsh_family = falconn.LSHFamily.CrossPolytope\n",
        "    params_cp.distance_function = falconn.DistanceFunction.EuclideanSquared\n",
        "    params_cp.l = self.number_of_tables\n",
        "    params_cp.num_rotations = 1\n",
        "    params_cp.seed = 5721840\n",
        "    params_cp.num_setup_threads = 0\n",
        "    params_cp.storage_hash_table = falconn.StorageHashTable.BitPackedFlatHashTable\n",
        "    falconn.compute_number_of_hash_functions(18, params_cp)\n",
        "    table = falconn.LSHIndex(params_cp)\n",
        "    table.setup(act - center) #changed\n",
        "    query_object = table.construct_query_object()\n",
        "\n",
        "    return query_object\n",
        "\n",
        "  def index_layers(self):\n",
        "    index_act = {}\n",
        "    center_arr = []\n",
        "    for layer in range(self.knn_layers_count):\n",
        "      center = np.mean(self.activations_train[layer], axis=0)\n",
        "      center_arr.append(center)\n",
        "      index_act[layer] = self.index_single_layer(self.activations_train[layer], center)\n",
        "    return index_act, center_arr\n",
        "\n",
        "  def query_index(self, index, activations, center):\n",
        "    # activations -= center\n",
        "    closest_labels = []\n",
        "\n",
        "    for (j, query) in enumerate(activations - center): # changed\n",
        "        a = index.find_k_nearest_neighbors(query, 75)\n",
        "        y_label = y_train[a]\n",
        "        y_label_2 = []\n",
        "        for i in range(y_label.shape[0]):\n",
        "            y_label_2.append(np.argmax(y_label[i]))\n",
        "        closest_labels.append(y_label_2)\n",
        "    return np.array(closest_labels)\n",
        "\n",
        "  def find_neighbours_test(self, index, center_arr):\n",
        "    try:\n",
        "      neighbours = load_pickle(self.folder + 'lsh/neighbours_lsh_new.pkl')\n",
        "      print('loaded neighbours from disc')\n",
        "    except FileNotFoundError:\n",
        "      print('failed to load neighbours index from disc')\n",
        "      neighbours = {}\n",
        "      for layer in range(self.knn_layers_count):\n",
        "        neighbours[layer] = {}\n",
        "        for eps in self.epsilons:\n",
        "          neighbours[layer][eps] = self.query_index(index[layer], self.activations_test[eps][layer], center_arr[layer])\n",
        "    return neighbours\n",
        "\n",
        "  def find_neighbours_calib(self, index):\n",
        "    neigbours_cal = {}\n",
        "    for layer in range(self.knn_layers_count):\n",
        "      center = np.mean(self.activations_calib[layer], axis=0)\n",
        "      neigbours_cal[layer] = self.query_index(index[layer], self.activations_calib[layer], center)\n",
        "\n",
        "    return neigbours_cal"
      ],
      "metadata": {
        "id": "q460UnDUkhCt"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DkNN"
      ],
      "metadata": {
        "id": "HHM-XaoGbj4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DkNN:\n",
        "  def __init__(self, ann_object):\n",
        "    self.model = ann_object.model\n",
        "    self.neighbours = ann_object.neighbours\n",
        "    self.neighbours_calib = ann_object.neighbours_calib\n",
        "    self.knn_layers_count = ann_object.knn_layers_count\n",
        "    self.epsilons = ann_object.epsilons\n",
        "\n",
        "    nonconformity_calib = self.calibrate_nonconformity()\n",
        "    p_values, knn_predicted_labels = self.calculate_performance_parameters(nonconformity_calib)\n",
        "\n",
        "    for eps in self.epsilons:\n",
        "      self.plot_reliability('DkNN', eps, p_values[eps], knn_predicted_labels[eps])\n",
        "\n",
        "  # Returns how many neighbours does not match real label\n",
        "  def count_not_matching_labels(self, neihgbours_arr):\n",
        "    nonconformity = []\n",
        "    for i in range(0, neihgbours_arr.shape[0]):\n",
        "      incorrect = np.sum(neihgbours_arr[i] != y_cal_raw[i])\n",
        "      nonconformity.append(incorrect)\n",
        "\n",
        "    return np.array(nonconformity)\n",
        "\n",
        "  def calibrate_nonconformity(self):\n",
        "    nonconformity_calib = np.zeros(self.neighbours_calib[0].shape[0])\n",
        "    for layer in range(self.knn_layers_count):\n",
        "      nonconformity_calib += self.count_not_matching_labels(self.neighbours_calib[layer])\n",
        "\n",
        "    # sort\n",
        "    nonconformity_calib =  np.sort(nonconformity_calib)\n",
        "    # trim zeros\n",
        "    nonconformity_calib = np.trim_zeros(nonconformity_calib, trim='f')\n",
        "\n",
        "    return nonconformity_calib\n",
        "\n",
        "  # calculate_nonconformity for each class based on calibration\n",
        "  def calculate_nonconformity(self, eps):\n",
        "    nonconformity_for_class = np.full((x_test.shape[0], num_classes), K_NEIGHBOURS * self.knn_layers_count, dtype=np.float32)\n",
        "    for i in range(x_test.shape[0]):\n",
        "      for layer in range(self.knn_layers_count):\n",
        "        for neighbour in self.neighbours[layer][eps][i]:\n",
        "          nonconformity_for_class[i][neighbour] -= 1\n",
        "\n",
        "    return nonconformity_for_class\n",
        "\n",
        "  def calculate_p_values(self, nonconformity_for_class, nonconformity):\n",
        "    p_values = np.empty((x_test.shape[0], num_classes),  dtype=np.float32)\n",
        "    for i in range(x_test.shape[0]):\n",
        "      for j in range(num_classes):\n",
        "        insert_index = bisect_left(nonconformity, nonconformity_for_class[i][j])\n",
        "        p_values[i][j] = (nonconformity.shape[0] - insert_index) / nonconformity.shape[0]\n",
        "    return p_values\n",
        "\n",
        "  def predict_labels(self, nonconformity_for_class):\n",
        "    knn_predicted_labels = []\n",
        "    for i in range(x_test.shape[0]):\n",
        "      knn_predicted_labels.append(np.argmin(nonconformity_for_class[i]))\n",
        "    return np.array(knn_predicted_labels)\n",
        "\n",
        "  def calculate_performance_parameters(self, nonconformity_calib):\n",
        "    nonconformity_for_class = {}\n",
        "    p_values = {}\n",
        "    knn_predicted_labels = {}\n",
        "    for eps in self.epsilons:\n",
        "      nonconformity_for_class = self.calculate_nonconformity(eps)\n",
        "      p_values[eps] = self.calculate_p_values(nonconformity_for_class, nonconformity_calib)\n",
        "      knn_predicted_labels[eps] = self.predict_labels(nonconformity_for_class)\n",
        "    return p_values, knn_predicted_labels\n",
        "\n",
        "  def calculate_performance_per_cred(self, confidence, predicted_labels):\n",
        "    credibility = np.max(confidence, axis=1)\n",
        "    distribution = np.zeros(10)\n",
        "    correct_labels = np.zeros(10)\n",
        "\n",
        "    for i in range(credibility.shape[0]):\n",
        "      bin = credibility[i] // 0.1 / 10\n",
        "      bin_index = int(bin*10)\n",
        "      distribution[bin_index] += 1\n",
        "      if (predicted_labels[i] == y_test_raw[i]):\n",
        "        correct_labels[bin_index] += 1\n",
        "\n",
        "    for i in range(correct_labels.shape[0]):\n",
        "      if correct_labels[i] != 0 and distribution[i] != 0:\n",
        "        correct_labels[i] /=  distribution[i]\n",
        "\n",
        "    return distribution, correct_labels \n",
        "\n",
        "  def plot_reliability(self, model_type, eps, confidence, predicted_labels):\n",
        "\n",
        "    if model_type == 'Softmax':\n",
        "      softmax_probabilities = model1.predict(model1adv.fgsm_adversarials[eps])\n",
        "      softmax_classes = softmax_probabilities.argmax(axis=-1)\n",
        "      confidence = softmax_probabilities\n",
        "      num_points, reliability_diag = self.calculate_performance_per_cred(confidence, softmax_classes)\n",
        "    else:\n",
        "      num_points, reliability_diag = self.calculate_performance_per_cred(confidence, predicted_labels)\n",
        "      print(np.array(num_points))\n",
        "      print(np.array(reliability_diag))\n",
        "\n",
        "\n",
        "    bars_begin = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "    bars_end = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
        "    bars_center = [0.05, 0.15, 0.25, 0.35, 0.45, 0.55, 0.65, 0.75, 0.85, 0.95]\n",
        "\n",
        "\n",
        "    fig, ax1 = plt.subplots()\n",
        "    p1 = ax1.bar(bars_center, np.round(reliability_diag*100, 1), width=.1, alpha=0.8, edgecolor = \"black\")\n",
        "\n",
        "    ax2 = ax1.twinx()\n",
        "    ax2.plot(bars_center, num_points, color='r', linestyle='-', linewidth=6.0)\n",
        "    ax1.set_ylim([0, 100])\n",
        "\n",
        "    plt.title(\"Reliability Diagram: \" + model_type + ', eps: ' + str(eps))\n",
        "    ax2.set_ylabel('Number of points in dataset', color='r')\n",
        "    ax1.set_xlabel('Prediction Credibility %')\n",
        "    ax1.set_ylabel('Prediction Accuracy %')\n",
        "    ax2.tick_params(colors='r')\n",
        "    # ax1.bar_label(p1, label_type='center', fmt='%.1f%%',  weight='bold')\n",
        "    \n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "PWr17XJgbmLZ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # def test_model_accuracy(labels1, labels2):\n",
        "#   correct_labels = np.sum(labels1 == labels2)\n",
        "#   return correct_labels / labels1.shape[0] * 100\n",
        "\n",
        "# def accuracy_per_model(knn_predicted_labels):\n",
        "#   accuracies_plot_knn = []\n",
        "#   for eps in epsilons:\n",
        "#     model_accuracy = test_model_accuracy(knn_predicted_labels[eps], y_test_raw)\n",
        "#     accuracies_plot_knn.append(model_accuracy)\n",
        "#   return accuracies_plot_knn\n",
        "\n",
        "# accuracies_plot_knn = accuracy_per_model(knn_predicted_labels)\n",
        "# plot_accuracy_vs_epsilon(model, fgsm_adversarials)\n",
        "# plt.plot(epsilons, accuracies_plot_knn, linestyle='--', marker='^', color='r', label = 'DkNN - NND')\n",
        "# plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "02hxPftmqzF1"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Test Model 1**"
      ],
      "metadata": {
        "id": "tqjoZ-r_cRFG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 1"
      ],
      "metadata": {
        "id": "Vrp58uhu3jwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if False:\n",
        "  model1adv = Adversaries(\n",
        "      model = model1,\n",
        "      epsilons = [0.15],\n",
        "      folder = 'model1_three_lay/')\n",
        "\n",
        "  activations_ttc = datasets_activations(model1, model1adv, layers_activations_small, neighbours_layers_indexes)\n",
        "\n",
        "  test_falconn_m1 = False\n",
        "  test_pynnd_m1 = True\n",
        "  test_annoy_m1 = False\n",
        "\n",
        "  if test_falconn_m1:\n",
        "    falconn_m1 = Falconn(\n",
        "        model1,\n",
        "        activations_ttc,\n",
        "        neighbours_layers_indexes,\n",
        "        model1adv.epsilons,\n",
        "        'model1_three_lay/')\n",
        "    dknn_falconn_m1 = DkNN(falconn_m1)\n",
        "\n",
        "  if test_pynnd_m1:\n",
        "    pynnd_m1 = PyNND(\n",
        "        model1,\n",
        "        activations_ttc,\n",
        "        neighbours_layers_indexes,\n",
        "        model1adv.epsilons,\n",
        "        'model1_three_lay/'\n",
        "        )\n",
        "    dknn_pynnd_m1 = DkNN(pynnd_m1)\n",
        "\n",
        "  if test_annoy_m1:\n",
        "    annoy_m1 = Annoy(\n",
        "        model1,\n",
        "        activations_ttc,\n",
        "        neighbours_layers_indexes,\n",
        "        model1adv.epsilons,\n",
        "        'model1_three_lay/')\n",
        "\n",
        "    dknn_annoy_m1 = DkNN(annoy_m1)"
      ],
      "metadata": {
        "id": "v4nLclCcmTKo"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Test model 2**"
      ],
      "metadata": {
        "id": "noUd5gSm3p5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model2adv = Adversaries(\n",
        "    model = model2,\n",
        "    epsilons = [0.15],\n",
        "    folder = 'model2/experiment2/')\n",
        "\n",
        "activations_ttc2 = datasets_activations(\n",
        "    model2, \n",
        "    model2adv, \n",
        "    layers_activations_big_data, \n",
        "    neighbours_layers_indexes2)"
      ],
      "metadata": {
        "id": "UjngWYFO3ibR"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pynnd_m1 = PyNND(\n",
        "    model1,\n",
        "    activations_ttc2,\n",
        "    neighbours_layers_indexes,\n",
        "    model2adv.epsilons,\n",
        "    'model2/experiment2/'\n",
        "    )\n",
        "\n",
        "dknn_pynnd_m1 = DkNN(pynnd_m1)"
      ],
      "metadata": {
        "id": "j7XHnYze4w5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Faiss"
      ],
      "metadata": {
        "id": "7AdRK5UIM-Gf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# neighbours_index_faiss = {}\n",
        "# dim = activations_train[layer].shape[1]\n",
        "# ind = faiss.IndexLSH(dim, dim * 2)\n",
        "# for layer in range(knn_layers_count):\n",
        "#   neighbours_index_faiss[layer] = faiss.IndexLSH(activations_train[layer].shape[1], dim * 4)\n",
        "#   neighbours_index_faiss[layer].add(activations_train[layer])"
      ],
      "metadata": {
        "id": "xFG9hGqKU5BY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# neighbors_faiss = {}\n",
        "# for layer in range(knn_layers_count):\n",
        "#   neighbors_faiss[layer] = {}\n",
        "#   for eps in epsilons:\n",
        "#     neighbors_faiss[layer][eps] = neighbours_index_faiss[layer].search(activations[eps][layer], 75)[0]"
      ],
      "metadata": {
        "id": "zpVOj8kwVOmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "FvElOaiLaHQc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nmslib"
      ],
      "metadata": {
        "id": "f52vsA_LaIqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install nmslib\n",
        "# import nmslib"
      ],
      "metadata": {
        "id": "keIureYgbiAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# index_nmslib = nmslib.init(method='hnsw', space='cosinesimil')\n",
        "# index_nmslib.addDataPointBatch(activations_train[0])\n",
        "# index_nmslib.createIndex({'post': 2}, print_progress=True)\n",
        "# ids, distances = index_nmslib.knnQuery(activations[0.0][0], k=75)"
      ],
      "metadata": {
        "id": "O9g8mIa4cvO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Faiss 2"
      ],
      "metadata": {
        "id": "FBiM1oYmbpxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class IVPQIndex():\n",
        "#     def __init__(self, vectors, labels):\n",
        "#         self.dimension = vectors.shape[1]\n",
        "#         self.vectors = vectors.astype('float32')\n",
        "#         self.labels = labels    \n",
        "#         def build(self, number_of_partition=8, search_in_x_partitions=2, subvector_size=8):\n",
        "#           quantizer = faiss.IndexFlatL2(self.dimension)\n",
        "#           self.index = faiss.IndexIVFPQ(quantizer, \n",
        "#                                         self.dimension, \n",
        "#                                         number_of_partition, \n",
        "#                                         search_in_x_partitions, \n",
        "#                                         subvector_size)\n",
        "#           self.index.train(self.vectors)\n",
        "#           self.index.add(self.vectors)\n",
        "        \n",
        "#     def query(self, vectors, k=10):\n",
        "#         distances, indices = self.index.search(vectors, k) \n",
        "#         # I expect only query on one vector thus the slice\n",
        "#         return [self.labels[i] for i in indices[0]]"
      ],
      "metadata": {
        "id": "g1Vq944SaL4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# index_quan = faiss.IndexFlatL2(activations_train[layer].shape[1])"
      ],
      "metadata": {
        "id": "EtxLsJwEaNkg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "hbfHUacwXai7",
        "sauhy9NhkdfL",
        "HHM-XaoGbj4O",
        "7AdRK5UIM-Gf",
        "f52vsA_LaIqr",
        "FBiM1oYmbpxd",
        "zX-XkLDgVqvT"
      ],
      "machine_shape": "hm",
      "name": "base.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}