{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Topolewski-Kamil/dKnn/blob/main/base.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Deep k-Nearest Neighbours**"
      ],
      "metadata": {
        "id": "845beWVYyBE1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bimNvVLP0XD"
      },
      "source": [
        "Mount drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLe7HBrK7CbQ",
        "outputId": "3b1c3cd7-e50d-4c08-e999-cdcfeeb77c6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/disseration\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/My Drive/disseration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJn05KsnfDLI"
      },
      "source": [
        "Import modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SnZbGpex18E5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbc84e22-2f90-4cf0-8403-9f0047ff702d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pynndescent\n",
            "  Downloading pynndescent-0.5.6.tar.gz (1.1 MB)\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |▋                               | 20 kB 22.5 MB/s eta 0:00:01\r\u001b[K     |▉                               | 30 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 51 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 61 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 81 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 92 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 102 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 112 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 122 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 133 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████                            | 143 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 153 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 163 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████                           | 174 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 184 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 194 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 204 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████                          | 215 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 225 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 235 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████                         | 245 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 256 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 266 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 276 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████                        | 286 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 296 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 307 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 317 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 327 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 337 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 348 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 358 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 368 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 378 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 389 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 399 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 409 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 419 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 430 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 440 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 450 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 460 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 471 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 481 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 491 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 501 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 512 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 522 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 532 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 542 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 552 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 563 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 573 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 583 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 593 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 604 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 614 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 624 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 634 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 645 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 655 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 665 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 675 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 686 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 696 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 706 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 716 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 727 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 737 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 747 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 757 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 768 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 778 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 788 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 798 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 808 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 819 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 829 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 839 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 849 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 860 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 870 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 880 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 890 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 901 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 911 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 921 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 931 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 942 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 952 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 962 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 972 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 983 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 993 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.0 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.0 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.0 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.0 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.0 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.1 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.1 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.1 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.1 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.1 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.1 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.1 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1 MB 8.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.7/dist-packages (from pynndescent) (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from pynndescent) (1.4.1)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.7/dist-packages (from pynndescent) (0.51.2)\n",
            "Requirement already satisfied: llvmlite>=0.30 in /usr/local/lib/python3.7/dist-packages (from pynndescent) (0.34.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from pynndescent) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from numba>=0.51.2->pynndescent) (1.21.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.51.2->pynndescent) (57.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->pynndescent) (3.1.0)\n",
            "Building wheels for collected packages: pynndescent\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.6-py3-none-any.whl size=53943 sha256=0bbdb4c3e4853ef660a98bc972cd2a936b2b556cecd9f806fa39ce61648151b7\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/f1/56/f80d72741e400345b5a5b50ec3d929aca581bf45e0225d5c50\n",
            "Successfully built pynndescent\n",
            "Installing collected packages: pynndescent\n",
            "Successfully installed pynndescent-0.5.6\n",
            "Collecting falconn\n",
            "  Downloading FALCONN-1.3.1.tar.gz (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 9.6 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: falconn\n",
            "  Building wheel for falconn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for falconn: filename=FALCONN-1.3.1-cp37-cp37m-linux_x86_64.whl size=10582881 sha256=a0606e4f0138968353efb6572d8d112e18fdc3cf24342a6adb8ada66c667571f\n",
            "  Stored in directory: /root/.cache/pip/wheels/94/91/57/5ba08148df673f6c3cff96717120d7ad23a7493ef92040eb3e\n",
            "Successfully built falconn\n",
            "Installing collected packages: falconn\n",
            "Successfully installed falconn-1.3.1\n",
            "Collecting annoy\n",
            "  Downloading annoy-1.17.0.tar.gz (646 kB)\n",
            "\u001b[K     |████████████████████████████████| 646 kB 8.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: annoy\n",
            "  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for annoy: filename=annoy-1.17.0-cp37-cp37m-linux_x86_64.whl size=391585 sha256=f3a10689444ba039cb01c62fec621b31e9387b7951516913787ba2def35fbfd9\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/e8/1e/7cc9ebbfa87a3b9f8ba79408d4d31831d67eea918b679a4c07\n",
            "Successfully built annoy\n",
            "Installing collected packages: annoy\n",
            "Successfully installed annoy-1.17.0\n",
            "Collecting faiss-gpu\n",
            "  Downloading faiss_gpu-1.7.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 85.5 MB 68 kB/s \n",
            "\u001b[?25hInstalling collected packages: faiss-gpu\n",
            "Successfully installed faiss-gpu-1.7.2\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "## version for plotting bar labels (unstable)\n",
        "# !pip install matplotlib --upgrade \n",
        "\n",
        "## version for the rest of plots (stable)\n",
        "# !pip install matplotlib==3.1.3 \n",
        "\n",
        "# Standard python libraries\n",
        "import numpy as np\n",
        "import string\n",
        "import copy\n",
        "from bisect import bisect_left\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# tensorflow libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras import backend as K\n",
        "from keras.models import load_model, Sequential\n",
        "from keras.losses import MSE\n",
        "from keras.layers import Conv2D, Dense, Dropout, Flatten, MaxPooling2D, BatchNormalization\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "# approximate nearest neighbours libraries\n",
        "!pip install pynndescent\n",
        "!pip install falconn\n",
        "!pip install annoy\n",
        "!pip install faiss-gpu\n",
        "import faiss\n",
        "import pynndescent\n",
        "import falconn\n",
        "import annoy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_pickle(save_object, filename):\n",
        "  with open(filename, 'wb') as f:\n",
        "    pickle.dump(save_object, f)\n",
        "\n",
        "def load_pickle(filename):\n",
        "  with open(filename, 'rb') as f:\n",
        "    loaded_object = pickle.load(f)\n",
        "    return loaded_object"
      ],
      "metadata": {
        "id": "k6FvfQiGxqkA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvbVxy5tfCwQ"
      },
      "source": [
        "Import and preprocess MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hXAsY2jo1_y7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a82ac7e6-2f11-41c4-c4f4-d36e732e9d90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# number of classes\n",
        "num_classes = 10 \n",
        "\n",
        "# input shape of images fed into CNN\n",
        "input_shape = (28, 28, 1)\n",
        "\n",
        "# import mnist data\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Scale to [0, 1] size\n",
        "x_train = x_train.astype(\"float32\") / 255\n",
        "x_test = x_test.astype(\"float32\") / 255\n",
        "\n",
        "# Make images shape (28, 28, 1)\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "\n",
        "# keep raw labels in separate array\n",
        "y_test_raw = y_test[750:10000]\n",
        "y_train_raw = y_train\n",
        "y_cal_raw = y_test[0:750]\n",
        "\n",
        "# convert labels to one hot enconding\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "# split test set into calibration set\n",
        "x_cal = x_test[0:750] \n",
        "y_cal = y_test[0:750]\n",
        "x_test = x_test[750:10000]\n",
        "y_test = y_test[750:10000]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model 1** - recreation of N. Papernot and P. McDaniel model "
      ],
      "metadata": {
        "id": "Ysyk2wys5Glv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNU51mw7fU3A"
      },
      "source": [
        "Initialize CNN model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model architechture\n",
        "model1 = Sequential(\n",
        "    [\n",
        "        keras.Input(shape=input_shape),\n",
        "        Conv2D(64, kernel_size=(8, 8), strides=(2,2), padding='same', activation='relu', input_shape=x_train.shape[1:]),\n",
        "        Conv2D(128, kernel_size=(6, 6), strides=(2,2), padding='valid', activation='relu'),\n",
        "        Conv2D(128, kernel_size=(5, 5), strides=(1,1), padding='valid', activation='relu'),\n",
        "        Flatten(),\n",
        "        Dropout(0.5),\n",
        "        Dense(10, activation=tf.nn.softmax),\n",
        "    ]\n",
        ")\n",
        "\n",
        "batch_size = 500\n",
        "epochs = 8\n",
        "\n",
        "adam_opt = keras.optimizers.Adam(learning_rate=0.001)\n",
        "model1.compile(loss=\"categorical_crossentropy\", optimizer=adam_opt, metrics=[\"accuracy\"])\n",
        "\n",
        "# indexes of layers that will be indexed and used for querying nearest neibouts\n",
        "neighbours_layers_indexes = [0,1,2,5]\n",
        "knn_layers_count = len(neighbours_layers_indexes)\n",
        "\n",
        "# number of neigbours used for Aproximate Nearest Neighbour Algorithm\n",
        "K_NEIGHBOURS = 75"
      ],
      "metadata": {
        "id": "IrTFfvZfd8t1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train and evaluate model"
      ],
      "metadata": {
        "id": "z12frsoO9yZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# try loading model from files\n",
        "try:\n",
        "  model1 = load_model('model1_three_lay/modelA.h5')\n",
        "  print(\"loaded model from disc\")\n",
        "  \n",
        "\n",
        "# otherwise train from scratch\n",
        "except OSError:\n",
        "  print(\"failed to load model from disc\")\n",
        "  model1.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
        "  # model.save('deepKNN_model_new.h5')  # creates a HDF5 file\n",
        "    \n",
        "score = model1.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test accuracy:\", score[1])"
      ],
      "metadata": {
        "id": "IO3fDipN9yIs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2c7398e-8bbe-4300-f276-79c482da3a4a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "failed to load model from disc\n",
            "Epoch 1/8\n",
            "108/108 [==============================] - 12s 18ms/step - loss: 0.4407 - accuracy: 0.8638 - val_loss: 0.0880 - val_accuracy: 0.9735\n",
            "Epoch 2/8\n",
            "108/108 [==============================] - 2s 16ms/step - loss: 0.1246 - accuracy: 0.9641 - val_loss: 0.0585 - val_accuracy: 0.9835\n",
            "Epoch 3/8\n",
            "108/108 [==============================] - 2s 14ms/step - loss: 0.0837 - accuracy: 0.9758 - val_loss: 0.0515 - val_accuracy: 0.9850\n",
            "Epoch 4/8\n",
            "108/108 [==============================] - 2s 14ms/step - loss: 0.0677 - accuracy: 0.9804 - val_loss: 0.0452 - val_accuracy: 0.9870\n",
            "Epoch 5/8\n",
            "108/108 [==============================] - 2s 14ms/step - loss: 0.0532 - accuracy: 0.9841 - val_loss: 0.0372 - val_accuracy: 0.9907\n",
            "Epoch 6/8\n",
            "108/108 [==============================] - 1s 12ms/step - loss: 0.0435 - accuracy: 0.9870 - val_loss: 0.0414 - val_accuracy: 0.9897\n",
            "Epoch 7/8\n",
            "108/108 [==============================] - 1s 12ms/step - loss: 0.0367 - accuracy: 0.9887 - val_loss: 0.0382 - val_accuracy: 0.9888\n",
            "Epoch 8/8\n",
            "108/108 [==============================] - 1s 14ms/step - loss: 0.0319 - accuracy: 0.9899 - val_loss: 0.0329 - val_accuracy: 0.9907\n",
            "Test accuracy: 0.9914594888687134\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model 2** - my *Model*"
      ],
      "metadata": {
        "id": "h-vRkjY913XP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2jKKspx13XW"
      },
      "source": [
        "Initialize CNN model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model architechture\n",
        "model2 = Sequential(\n",
        "    [\n",
        "        keras.Input(shape=input_shape),\n",
        "        Conv2D(filters=64, kernel_size = (3,3), activation=\"relu\", input_shape=(28,28,1)),\n",
        "        Conv2D(filters=64, kernel_size = (3,3), activation=\"relu\"),\n",
        "        MaxPooling2D(pool_size=(2,2)),\n",
        "        BatchNormalization(),\n",
        "     \n",
        "        Conv2D(filters=128, kernel_size = (3,3), activation=\"relu\"),\n",
        "        Conv2D(filters=128, kernel_size = (3,3), activation=\"relu\"),\n",
        "        MaxPooling2D(pool_size=(2,2)),\n",
        "        BatchNormalization(),   \n",
        "\n",
        "        Conv2D(filters=256, kernel_size = (3,3), activation=\"relu\"),\n",
        "        MaxPooling2D(pool_size=(2,2)),\n",
        "        BatchNormalization(),\n",
        "            \n",
        "        Flatten(),\n",
        "        Dense(512,activation=\"relu\"),\n",
        "        Dense(10,activation=\"softmax\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "batch_size2 = 500\n",
        "epochs2 = 8\n",
        "\n",
        "adam_opt = keras.optimizers.Adam(learning_rate=0.001)\n",
        "model1.compile(loss=\"categorical_crossentropy\", optimizer=adam_opt, metrics=[\"accuracy\"])\n",
        "\n",
        "# indexes of layers that will be indexed and used for querying nearest neibouts\n",
        "neighbours_layers_indexes2 = [2,6,9]\n",
        "knn_layers_count2 = len(neighbours_layers_indexes2)\n",
        "\n",
        "# number of neigbours used for Aproximate Nearest Neighbour Algorithm\n",
        "K_NEIGHBOURS = 75"
      ],
      "metadata": {
        "id": "TdmTPLYn7Qa5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train and evaluate model"
      ],
      "metadata": {
        "id": "0n2jNtnP3fKp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# try loading model from files\n",
        "try:\n",
        "  model2 = load_model('model2/experiment2/deepKNN_model9947.h5')\n",
        "  print(\"loaded model from disc\")\n",
        "\n",
        "# otherwise train from scratch\n",
        "except OSError:\n",
        "  print(\"failed to load model from disc\")\n",
        "  model2.fit(x_train, y_train, batch_size=batch_size2, epochs=epochs2, validation_split=0.1)\n",
        "  # model2.save('model2/deepKNN_model2.h5')  # creates a HDF5 file\n",
        "  print(\"saved model successfully\")\n",
        "\n",
        "score = model2.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Test accuracy:\", score[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Adlc1djg2Stp",
        "outputId": "4b6ea983-ddf7-42f0-ceee-732a85710c4b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded model from disc\n",
            "Test loss: 0.02564595453441143\n",
            "Test accuracy: 0.9947026968002319\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Activations"
      ],
      "metadata": {
        "id": "ITTFlCjbndjE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get activations for each dataset"
      ],
      "metadata": {
        "id": "koERwqrrnVRT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_activations(activations):\n",
        "  for layer in activations:\n",
        "    layer /= np.linalg.norm(layer, axis=1).reshape(-1, 1)\n",
        "\n",
        "def layers_activations_small(model, dataset, neighbours_layers_indexes):\n",
        "  activations = []\n",
        "  for i in neighbours_layers_indexes: # 0,1,2-convolutions layer, 5-dense layer\n",
        "      layer = model.layers[i]\n",
        "      lay_act = K.function([model.layers[0].input], [layer.output])([dataset])[0]\n",
        "      print(lay_act.shape)\n",
        "      if i != 5:\n",
        "        activations.append(lay_act.reshape(lay_act.shape[0], lay_act.shape[1]*lay_act.shape[2]*lay_act.shape[3]))\n",
        "      else: \n",
        "        activations.append(lay_act)\n",
        "\n",
        "  normalize_activations(activations)\n",
        "  return activations\n",
        "\n",
        "def layers_activations_big_data(model, dataset, neighbours_layers_indexes):\n",
        "  activations = []\n",
        "\n",
        "  for i in neighbours_layers_indexes: # 0,1,2-convolutions layer, 5-dense layer\n",
        "      layer = model.layers[i]\n",
        "      lay_act_arr = []\n",
        "      chunk_size = 10000\n",
        "      for i in range(0, 10000, chunk_size):\n",
        "        lay_act = K.function([model.layers[0].input], [layer.output])([dataset[i:i+chunk_size]])[0]\n",
        "        lay_act_arr = np.array(lay_act)\n",
        "      for i in range(10000, len(dataset), chunk_size):\n",
        "        lay_act = K.function([model.layers[0].input], [layer.output])([dataset[i:i+chunk_size]])[0]\n",
        "        lay_act_arr = np.append(lay_act_arr, lay_act, axis=0)\n",
        "\n",
        "      activations.append(lay_act_arr.reshape(lay_act_arr.shape[0], lay_act_arr.shape[1]*lay_act_arr.shape[2]*lay_act_arr.shape[3]))\n",
        "      # reshaped_output_layer.append(lay_act_arr)\n",
        "\n",
        "  normalize_activations(activations)\n",
        "  return activations\n",
        "\n",
        "def datasets_activations(model, model_adv, layers_activations, neighbours_layers_indexes):\n",
        "  activations_train = layers_activations(model, x_train, neighbours_layers_indexes)\n",
        "\n",
        "  activations_test = {}\n",
        "  for eps in model_adv.epsilons:\n",
        "    activations_test[eps] = layers_activations(model, model_adv.fgsm_adversaries[eps], neighbours_layers_indexes)\n",
        "\n",
        "  activations_calib = layers_activations(model, x_cal, neighbours_layers_indexes)\n",
        "\n",
        "  return [activations_train, activations_test, activations_calib]"
      ],
      "metadata": {
        "id": "j4gdg2ey2Z8u"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adversarial Examples"
      ],
      "metadata": {
        "id": "WH6LaitFAB9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## This class provides functionality for generating adversairial examples\n",
        "class Adversaries:\n",
        "  def __init__(self, model, epsilons, folder):\n",
        "    self.model = model\n",
        "    self.epsilons = copy.deepcopy(epsilons)\n",
        "    self.folder = folder\n",
        "    self.fgsm_adversaries = self.get_adversaries()\n",
        "\n",
        "  def load_adversaries(self):\n",
        "    adversaries = {}\n",
        "    for eps in self.epsilons:\n",
        "      eps_str = str(eps)\n",
        "      eps_str = eps_str.translate(str.maketrans('', '', string.punctuation))\n",
        "      path = self.folder + 'adv_datasets/adversaries' + eps_str  + '.npy'\n",
        "      adversaries[eps] = np.load(path)\n",
        "    return adversaries\n",
        "\n",
        "  # function saving adversarials examples\n",
        "  def save_adversaries(self, adversaries):\n",
        "    for eps in self.epsilons:\n",
        "      eps_str = str(eps)\n",
        "      eps_str = eps_str.translate(str.maketrans('', '', string.punctuation))\n",
        "      filename = self.folder + 'adv_datasets/adversaries' + eps_str + '.npy'\n",
        "      np.save(filename, adversaries[eps])\n",
        "      print('saved ' + str(eps))\n",
        "\n",
        "  def generate_single_adversary(self, image, label, eps):\n",
        "\n",
        "    # Make images shape (1, 28, 28, 1)\n",
        "    image = tf.cast(image.reshape(1, 28, 28, 1), tf.float32)\n",
        "\n",
        "    # record our gradients\n",
        "    with tf.GradientTape() as tape:\n",
        "\n",
        "      # start tracing image by the Tape\n",
        "      tape.watch(image)\n",
        "\n",
        "      #compute prediction\n",
        "      pred = self.model(image)\n",
        "\n",
        "      # compute loss\n",
        "      loss = MSE(label, pred)\n",
        "\n",
        "      # calculate the gradient of loss function \n",
        "      grad = tape.gradient(loss, image)\n",
        "\n",
        "      # compute the sign of the gradient\n",
        "      sign = tf.sign(grad)\n",
        "\n",
        "      # create perturbation\n",
        "      perturbation = sign * eps\n",
        "\n",
        "      # apply perturbation to image\n",
        "      adversary = (image + perturbation).numpy()\n",
        "\n",
        "      # Make images shape (28, 28, 1)\n",
        "      adversary = adversary.reshape(28, 28, 1)\n",
        "      \n",
        "      return adversary\n",
        "\n",
        "  def get_adversaries(self):\n",
        "    try:\n",
        "      adversaries = self.load_adversaries()\n",
        "      self.epsilons.insert(0, 0.0)\n",
        "      adversaries[0.0] = x_test\n",
        "      print('loaded adversarial datasets from disc')\n",
        "    except FileNotFoundError:\n",
        "      print('failed to load adversarial datasets from disc')\n",
        "      adversaries = {}\n",
        "      for eps in self.epsilons:\n",
        "        adv = []\n",
        "        for i in range(x_test.shape[0]):\n",
        "          adv.append(self.generate_single_adversary(x_test[i], y_test[i], eps))\n",
        "        adversaries[eps] = np.array(adv)\n",
        "        # self.save_adversaries(adversaries)\n",
        "      self.epsilons.insert(0, 0.0)\n",
        "      adversaries[0.0] = x_test\n",
        "    return adversaries\n",
        "\n",
        "  def plot_accuracy_vs_epsilon(self):\n",
        "    accuracies_plot = []\n",
        "    for eps in self.epsilons:\n",
        "      adversary_score = self.model.evaluate(self.fgsm_adversaries[eps], y_test, verbose=0)\n",
        "      accuracies_plot.append(adversary_score[1] * 100)\n",
        "      print(adversary_score)\n",
        "\n",
        "    plt.figure(figsize=(5,5))\n",
        "    plt.plot(self.epsilons, accuracies_plot, linestyle='--', marker='o', color='b', label = 'DNN')\n",
        "\n",
        "    plt.yticks(np.arange(0, 101, step=10))\n",
        "    plt.xticks(np.arange(0, 0.16, step=0.025))\n",
        "    plt.xlabel(\"Epsilon\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.title(\"Accuracy vs Epsilon\")\n",
        "    plt.legend()\n",
        "    plt.savefig(self.folder + 'images/adv_vs_eps.png')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "  def plot_adv_examples(self):\n",
        "    counter = 0\n",
        "    plt.figure(figsize=(10,15))\n",
        "\n",
        "    for eps in [0.025, 0.1, 0.15]:\n",
        "      adv_img = self.fgsm_adversaries[eps][10:15]\n",
        "      true_label = y_test[10:15]\n",
        "\n",
        "      adv_results = model1.predict(self.fgsm_adversaries[eps][10:15])\n",
        "      predicted_labels = []\n",
        "      for result in adv_results:\n",
        "        predicted_labels.append(np.argmax(result))\n",
        "\n",
        "      for i in range(5):\n",
        "          counter += 1\n",
        "          plt.subplot(len(self.epsilons), 5, counter)\n",
        "          plt.xticks([], [])\n",
        "          plt.yticks([], [])\n",
        "\n",
        "          if i == 0:\n",
        "            plt.ylabel(r'$\\bf{Eps: '+ str(eps) + '}$', fontsize=14)\n",
        "\n",
        "          label = np.argmax(true_label[i])\n",
        "          if predicted_labels[i] != label:\n",
        "            incorrect_label = plt.title('Pred. label: ' + str(predicted_labels[i]))\n",
        "            plt.setp(incorrect_label, color='r')\n",
        "          else:\n",
        "            if eps == 0.025:\n",
        "              plt.title(r'$\\bf{' + 'True label:' + str(label) + '}$' + '\\n\\nPred. label:' + str(predicted_labels[i]))\n",
        "            else:\n",
        "              plt.title('Pred. label: ' + str(predicted_labels[i]))\n",
        "\n",
        "          plt.imshow(np.squeeze(adv_img[i]), cmap='gray')\n",
        "    # plt.savefig(self.folder + 'images/adv_examples.png')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "zxdLmDW-AM-I"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyNND"
      ],
      "metadata": {
        "id": "hbfHUacwXai7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PyNND:\n",
        "  def __init__(self, model, activations_ttc, neighbours_layers_indexes, epsilons, folder):\n",
        "    self.model = model\n",
        "    self.neighbours_layers_indexes = neighbours_layers_indexes\n",
        "    self.activations_train = activations_ttc[0]\n",
        "    self.activations_test = activations_ttc[1]\n",
        "    self.activations_calib = activations_ttc[2]\n",
        "    self.knn_layers_count = len(neighbours_layers_indexes)\n",
        "    self.epsilons = epsilons\n",
        "    self.folder = folder\n",
        "    self.knn_eps = 0.1\n",
        "\n",
        "    index_act = self.index_layer()\n",
        "    self.neighbours_test = self.find_neighbours_test(index_act)\n",
        "    self.neighbours_calib = self.find_neighbours_calib(index_act)\n",
        "\n",
        "  def index_layer(self):\n",
        "    try: \n",
        "        index_act = load_pickle(self.folder + 'pynnd/neighbours_index.pkl')\n",
        "        print('loaded neighbours index from disc')\n",
        "    except FileNotFoundError:\n",
        "        print('failed to load neighbours index from disc')\n",
        "        index_act = {}\n",
        "        for layer in range(self.knn_layers_count):\n",
        "          index_act[layer] = pynndescent.NNDescent(self.activations_train[layer]) # index training data\n",
        "          index_act[layer].prepare() # prepare for faster query\n",
        "        # save_pickle(index_act, self.folder + 'neighbours_index.pkl')\n",
        "    return index_act\n",
        "\n",
        "  def query_index(self, index, activations):\n",
        "    neighbours = index.query(activations, k=K_NEIGHBOURS, epsilon=self.knn_eps)\n",
        "    neighbours_labels = y_train_raw[neighbours[0]]\n",
        "    return neighbours_labels\n",
        "\n",
        "  def find_neighbours_test(self, index):\n",
        "    try:\n",
        "      neighbours = load_pickle(self.folder + 'pynnd/neighbours.pkl')\n",
        "      print('loaded neighbours from disc')\n",
        "    except FileNotFoundError:\n",
        "      print('failed to load neighbours from disc')\n",
        "      neighbours = {}\n",
        "      for layer in range(knn_layers_count):\n",
        "        neighbours[layer] = {}\n",
        "        for eps in self.epsilons:\n",
        "          neighbours[layer][eps] = self.query_index(index[layer], self.activations_test[eps][layer])\n",
        "      # save_pickle(neighbors, 'model1exact/neighbours.pkl')\n",
        "    return neighbours\n",
        "\n",
        "  def find_neighbours_calib(self, index):\n",
        "    neigbours_calib = {}\n",
        "    for layer in range(self.knn_layers_count):\n",
        "      neigbours_calib[layer] = self.query_index(index[layer], self.activations_calib[layer])\n",
        "    return neigbours_calib"
      ],
      "metadata": {
        "id": "SJlI8FAhzrVq"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Annoy"
      ],
      "metadata": {
        "id": "EkK5h6A8XezZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Annoy:\n",
        "  def __init__(self, model, activations_ttc, neighbours_layers_indexes, epsilons, folder):\n",
        "    self.model = model\n",
        "    self.neighbours_layers_indexes = neighbours_layers_indexes\n",
        "    self.activations_train = activations_ttc[0]\n",
        "    self.activations_test = activations_ttc[1]\n",
        "    self.activations_calib = activations_ttc[2]\n",
        "    self.epsilons = epsilons\n",
        "    self.folder = folder\n",
        "\n",
        "    self.knn_layers_count = len(neighbours_layers_indexes)\n",
        "\n",
        "    index_act = self.index_layer()\n",
        "    self.neighbours_test = self.find_neighbours_test(index_act)\n",
        "    self.neighbours_calib = self.find_neighbours_calib(index_act)\n",
        "\n",
        "  def save_annoy_index(self, neighbours):\n",
        "    for layer in range(self.knn_layers_count):\n",
        "      neighbours[layer].save(self.folder + 'annoy/index/annoy_index_' + str(layer) + '.ann')\n",
        "\n",
        "  def load_annoy_index(self):\n",
        "    index_act = {}\n",
        "    for layer in range(self.knn_layers_count):\n",
        "      index_act[layer] = annoy.AnnoyIndex(self.activations_train[layer].shape[1], 'angular')\n",
        "      index_act[layer].load(self.folder +'annoy/index/annoy_index_' + str(layer) + '.ann')\n",
        "    return index_act\n",
        "\n",
        "  def load_annoy_neighbours(self):\n",
        "    neighbors_annoy = {}\n",
        "    for layer in range(knn_layers_count):\n",
        "      neighbors_annoy[layer] = load_pickle(self.folder + 'annoy/neighbours/neighbours_annoy_layer_' + str(layer + 1) + '.pkl')\n",
        "      for eps in self.epsilons:\n",
        "        neighbors_annoy[layer][eps] = y_train_raw[neighbors_annoy[layer][eps]]\n",
        "    return neighbors_annoy\n",
        "  \n",
        "  def index_layer(self):\n",
        "    try: \n",
        "      index_act = self.load_annoy_index()\n",
        "      print('loaded annoy index successfully')\n",
        "    except OSError:\n",
        "      print('failed to load neighbours index from disc')\n",
        "      index_act = {}\n",
        "      for layer in range(self.knn_layers_count):\n",
        "        index_act[layer] = annoy.AnnoyIndex(self.activations_train[layer].shape[1], 'angular')\n",
        "\n",
        "      for layer in range(self.knn_layers_count):\n",
        "        for i in range(self.activations_train[layer].shape[0]):\n",
        "          index_act[layer].add_item(i, self.activations_train[layer][i])\n",
        "        index_act[layer].build(50) # build 50 trees\n",
        "\n",
        "      # self.save_annoy_index(index_act)\n",
        "    return index_act\n",
        "\n",
        "  def find_neighbours_test(self, index):\n",
        "    try:\n",
        "      neighbours = self.load_annoy_neighbours()\n",
        "      print('loaded annoy neighbours')\n",
        "    except OSError:\n",
        "      print('failed to loaded annoy neighbours')\n",
        "      neighbours = {}\n",
        "      for layer in range(self.knn_layers_count):\n",
        "        neighbours[layer] = {}\n",
        "        for eps in self.epsilons:\n",
        "          arr = []\n",
        "          for act in self.activations_test[eps][layer]:\n",
        "            indexes = index[layer].get_nns_by_vector(act, K_NEIGHBOURS)\n",
        "            arr.append(y_train_raw[indexes])\n",
        "          neighbours[layer][eps] = np.array(arr)\n",
        "    return neighbours\n",
        "\n",
        "  def find_neighbours_calib(self, index):\n",
        "    neigbours_cal = {}\n",
        "    for layer in range(self.knn_layers_count):\n",
        "      arr = []\n",
        "      for act in self.activations_calib[layer]:\n",
        "        calib_indexes = index[layer].get_nns_by_vector(act, K_NEIGHBOURS)\n",
        "        arr.append(y_train_raw[calib_indexes])\n",
        "      neigbours_cal[layer] = np.array(arr)\n",
        "    return neigbours_cal"
      ],
      "metadata": {
        "id": "chyfDRWNXiQS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Falconn"
      ],
      "metadata": {
        "id": "sauhy9NhkdfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Falconn:\n",
        "  def __init__(self, model, activations_ttc, neighbours_layers_indexes, epsilons, folder):\n",
        "    self.model = model\n",
        "    self.neighbours_layers_indexes = neighbours_layers_indexes\n",
        "    self.activations_train = activations_ttc[0]\n",
        "    self.activations_test = activations_ttc[1]\n",
        "    self.activations_calib = activations_ttc[2]\n",
        "    self.epsilons = epsilons\n",
        "    self.folder = folder\n",
        "    self.knn_layers_count = len(neighbours_layers_indexes)\n",
        "    self.number_of_tables = 50\n",
        "\n",
        "    index_act, center_arr = self.index_layers()\n",
        "    print('finished indexing')\n",
        "\n",
        "    self.neighbours_test = self.find_neighbours_test(index_act, center_arr)\n",
        "    self.neighbours_calib = self.find_neighbours_calib(index_act)\n",
        "\n",
        "  def index_single_layer(self, act, center):\n",
        "    params_cp = falconn.LSHConstructionParameters()\n",
        "    params_cp.dimension = len(act[0])\n",
        "    params_cp.lsh_family = falconn.LSHFamily.CrossPolytope\n",
        "    params_cp.distance_function = falconn.DistanceFunction.EuclideanSquared\n",
        "    params_cp.l = self.number_of_tables\n",
        "    params_cp.num_rotations = 1\n",
        "    params_cp.seed = 5721840\n",
        "    params_cp.num_setup_threads = 0\n",
        "    params_cp.storage_hash_table = falconn.StorageHashTable.BitPackedFlatHashTable\n",
        "    falconn.compute_number_of_hash_functions(18, params_cp)\n",
        "    table = falconn.LSHIndex(params_cp)\n",
        "    table.setup(act - center)\n",
        "    query_object = table.construct_query_object()\n",
        "\n",
        "    return query_object\n",
        "\n",
        "  def index_layers(self):\n",
        "    index_act = {}\n",
        "    center_arr = []\n",
        "    for layer in range(self.knn_layers_count):\n",
        "      center = np.mean(self.activations_train[layer], axis=0)\n",
        "      center_arr.append(center)\n",
        "      index_act[layer] = self.index_single_layer(self.activations_train[layer], center)\n",
        "    return index_act, center_arr\n",
        "\n",
        "  def query_index(self, index, activations, center):\n",
        "    # activations -= center\n",
        "    closest_labels = []\n",
        "\n",
        "    for (j, query) in enumerate(activations - center):\n",
        "        a = index.find_k_nearest_neighbors(query, 75)\n",
        "        y_label = y_train[a]\n",
        "        y_label_2 = []\n",
        "        for i in range(y_label.shape[0]):\n",
        "            y_label_2.append(np.argmax(y_label[i]))\n",
        "        closest_labels.append(y_label_2)\n",
        "    return np.array(closest_labels)\n",
        "\n",
        "  def find_neighbours_test(self, index, center_arr):\n",
        "    try:\n",
        "      neighbours = load_pickle(self.folder + 'lsh/neighbours_lsh_new.pkl')\n",
        "      print('loaded neighbours from disc')\n",
        "    except FileNotFoundError:\n",
        "      print('failed to load neighbours index from disc')\n",
        "      neighbours = {}\n",
        "      for layer in range(self.knn_layers_count):\n",
        "        neighbours[layer] = {}\n",
        "        for eps in self.epsilons:\n",
        "          neighbours[layer][eps] = self.query_index(index[layer], self.activations_test[eps][layer], center_arr[layer])\n",
        "    return neighbours\n",
        "\n",
        "  def find_neighbours_calib(self, index):\n",
        "    neigbours_cal = {}\n",
        "    for layer in range(self.knn_layers_count):\n",
        "      center = np.mean(self.activations_calib[layer], axis=0)\n",
        "      neigbours_cal[layer] = self.query_index(index[layer], self.activations_calib[layer], center)\n",
        "\n",
        "    return neigbours_cal"
      ],
      "metadata": {
        "id": "q460UnDUkhCt"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DkNN"
      ],
      "metadata": {
        "id": "HHM-XaoGbj4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DkNN:\n",
        "  def __init__(self, ann_object):\n",
        "    self.model = ann_object.model\n",
        "    self.neighbours_test = ann_object.neighbours_test\n",
        "    self.neighbours_calib = ann_object.neighbours_calib\n",
        "    self.knn_layers_count = ann_object.knn_layers_count\n",
        "    self.epsilons = ann_object.epsilons\n",
        "\n",
        "    nonconformity_calib = self.calibrate_nonconformity()\n",
        "    p_values, knn_predicted_labels = self.calculate_performance_params(nonconformity_calib)\n",
        "\n",
        "    for eps in self.epsilons:\n",
        "      self.plot_reliability('DkNN', eps, p_values[eps], knn_predicted_labels[eps])\n",
        "\n",
        "  # Returns how many neighbours does not match real label\n",
        "  def count_not_matching_labels(self, neihgbours_arr):\n",
        "    nonconformity = []\n",
        "    for i in range(0, neihgbours_arr.shape[0]):\n",
        "      incorrect = np.sum(neihgbours_arr[i] != y_cal_raw[i])\n",
        "      nonconformity.append(incorrect)\n",
        "\n",
        "    return np.array(nonconformity)\n",
        "\n",
        "  def calibrate_nonconformity(self):\n",
        "    nonconformity_calib = np.zeros(self.neighbours_calib[0].shape[0])\n",
        "    for layer in range(self.knn_layers_count):\n",
        "      nonconformity_calib += self.count_not_matching_labels(self.neighbours_calib[layer])\n",
        "\n",
        "    # sort\n",
        "    nonconformity_calib =  np.sort(nonconformity_calib)\n",
        "    # trim zeros\n",
        "    nonconformity_calib = np.trim_zeros(nonconformity_calib, trim='f')\n",
        "\n",
        "    return nonconformity_calib\n",
        "\n",
        "  # calculate_nonconformity for each class based on calibration\n",
        "  def calculate_nonconformity(self, eps):\n",
        "    nonconformity_for_class = np.full((x_test.shape[0], num_classes), K_NEIGHBOURS * self.knn_layers_count, dtype=np.float32)\n",
        "    for i in range(x_test.shape[0]):\n",
        "      for layer in range(self.knn_layers_count):\n",
        "        for neighbour in self.neighbours_test[layer][eps][i]:\n",
        "          nonconformity_for_class[i][neighbour] -= 1\n",
        "\n",
        "    return nonconformity_for_class\n",
        "\n",
        "  def calculate_p_values(self, nonconformity_for_class, nonconformity):\n",
        "    p_values = np.empty((x_test.shape[0], num_classes),  dtype=np.float32)\n",
        "    for i in range(x_test.shape[0]):\n",
        "      for j in range(num_classes):\n",
        "        insert_index = bisect_left(nonconformity, nonconformity_for_class[i][j])\n",
        "        p_values[i][j] = (nonconformity.shape[0] - insert_index) / nonconformity.shape[0]\n",
        "    return p_values\n",
        "\n",
        "  def predict_labels(self, nonconformity_for_class):\n",
        "    knn_predicted_labels = []\n",
        "    for i in range(x_test.shape[0]):\n",
        "      knn_predicted_labels.append(np.argmin(nonconformity_for_class[i]))\n",
        "    return np.array(knn_predicted_labels)\n",
        "\n",
        "  def calculate_performance_params(self, nonconformity_calib):\n",
        "    nonconformity_for_class = {}\n",
        "    p_values = {}\n",
        "    knn_predicted_labels = {}\n",
        "    for eps in self.epsilons:\n",
        "      nonconformity_for_class = self.calculate_nonconformity(eps)\n",
        "      p_values[eps] = self.calculate_p_values(nonconformity_for_class, nonconformity_calib)\n",
        "      knn_predicted_labels[eps] = self.predict_labels(nonconformity_for_class)\n",
        "    return p_values, knn_predicted_labels\n",
        "\n",
        "  def calculate_performance_per_cred(self, confidence, predicted_labels):\n",
        "    credibility = np.max(confidence, axis=1)\n",
        "    distribution = np.zeros(10)\n",
        "    correct_labels = np.zeros(10)\n",
        "\n",
        "    for i in range(credibility.shape[0]):\n",
        "      bin = credibility[i] // 0.1 / 10\n",
        "      bin_index = int(bin*10)\n",
        "      distribution[bin_index] += 1\n",
        "      if (predicted_labels[i] == y_test_raw[i]):\n",
        "        correct_labels[bin_index] += 1\n",
        "\n",
        "    for i in range(correct_labels.shape[0]):\n",
        "      if correct_labels[i] != 0 and distribution[i] != 0:\n",
        "        correct_labels[i] /=  distribution[i]\n",
        "\n",
        "    return distribution, correct_labels \n",
        "\n",
        "  def plot_reliability(self, model_type, eps, confidence, predicted_labels):\n",
        "\n",
        "    if model_type == 'Softmax':\n",
        "      softmax_probabilities = model1.predict(model1adv.fgsm_adversarials[eps])\n",
        "      softmax_classes = softmax_probabilities.argmax(axis=-1)\n",
        "      confidence = softmax_probabilities\n",
        "      num_points, reliability_diag = self.calculate_performance_per_cred(confidence, softmax_classes)\n",
        "    else:\n",
        "      num_points, reliability_diag = self.calculate_performance_per_cred(confidence, predicted_labels)\n",
        "      print(np.array(num_points))\n",
        "      print(np.array(reliability_diag))\n",
        "\n",
        "\n",
        "    bars_begin = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "    bars_end = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
        "    bars_center = [0.05, 0.15, 0.25, 0.35, 0.45, 0.55, 0.65, 0.75, 0.85, 0.95]\n",
        "\n",
        "\n",
        "    fig, ax1 = plt.subplots()\n",
        "    p1 = ax1.bar(bars_center, np.round(reliability_diag*100, 1), width=.1, alpha=0.8, edgecolor = \"black\")\n",
        "\n",
        "    ax2 = ax1.twinx()\n",
        "    ax2.plot(bars_center, num_points, color='r', linestyle='-', linewidth=6.0)\n",
        "    ax1.set_ylim([0, 100])\n",
        "\n",
        "    plt.title(\"Reliability Diagram: \" + model_type + ', eps: ' + str(eps))\n",
        "    ax2.set_ylabel('Number of points in dataset', color='r')\n",
        "    ax1.set_xlabel('Prediction Credibility %')\n",
        "    ax1.set_ylabel('Prediction Accuracy %')\n",
        "    ax2.tick_params(colors='r')\n",
        "    # ax1.bar_label(p1, label_type='center', fmt='%.1f%%',  weight='bold')\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "  def correct_neighbours_count(self, layer, eps):\n",
        "    correct_neighbours = [0] * (K_NEIGHBOURS + 1)\n",
        "    for i in range(self.neighbours_test[layer][eps].shape[0]):\n",
        "      correct = np.sum(self.neighbours_test[layer][eps][i] == y_test_raw[i])\n",
        "      correct_neighbours[correct] += 1\n",
        "    return correct_neighbours\n",
        "\n",
        "  def compare_neigbours(self, layer, eps):\n",
        "    correct_neighbours = self.correct_neighbours_count(layer, eps)\n",
        "    plt.figure(figsize=(15,5))\n",
        "    x_ax = np.arange(len(correct_neighbours))\n",
        "    # plt.bar(x_ax-0.2, self.neighbours[layer][0.0], 0.4, label = 'Epsilon: 0.0')\n",
        "    plt.bar(x_ax+0.2, correct_neighbours, 0.4, label = 'Epsilon: ' + str(eps))\n",
        "\n",
        "    plt.xlabel(\"Neigbours\")\n",
        "    plt.ylabel(\"Number of correct neigbours\")\n",
        "    plt.title(\"Number of correct neigbours on layer number: \" + str(layer+1))\n",
        "    plt.legend()\n",
        "\n",
        "    plt.xticks(x_ax);\n",
        "    plt.show() "
      ],
      "metadata": {
        "id": "PWr17XJgbmLZ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Test Model 1**"
      ],
      "metadata": {
        "id": "tqjoZ-r_cRFG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 1 - all layers"
      ],
      "metadata": {
        "id": "yLCixjPOC93F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "neighbours_layers_indexes = [0,1,2,5]\n",
        "knn_layers_count = len(neighbours_layers_indexes)\n",
        "\n",
        "model1adv = Adversaries(\n",
        "    model = model1,\n",
        "    epsilons = [0.025, 0.05, 0.075, 0.1, 0.125, 0.15],\n",
        "    folder = 'model1_exact/')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEdaOe7gC7nv",
        "outputId": "01f7acf8-6aca-41e3-eeee-56994ecda3af"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded adversarial datasets from disc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "activations_ttc_m1_4layer = datasets_activations(model1, model1adv, layers_activations_small, neighbours_layers_indexes)"
      ],
      "metadata": {
        "id": "UEMAkLOdPx12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_prob = model1.predict(x_test) \n",
        "y_prob_best = y_prob.max(axis=-1)\n",
        "np.average(y_prob_best)\n",
        "\n",
        "y_prob = model1.predict(model1adv.fgsm_adversaries[0.15]) \n",
        "y_prob_best = y_prob.max(axis=-1)\n",
        "\n",
        "np.average(y_prob_best)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcQOMI9hM1HB",
        "outputId": "1a64df71-8784-4d5d-af9c-5212d760fb94"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9921463"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pynnd_m1 = PyNND(\n",
        "    model1,\n",
        "    activations_ttc_m1_4layer,\n",
        "    neighbours_layers_indexes,\n",
        "    model1adv.epsilons,\n",
        "    'model1_exact/'\n",
        "    )\n",
        "\n",
        "dknn_pynnd_m1 = DkNN(pynnd_m1)"
      ],
      "metadata": {
        "id": "MH07ya-AEZUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a1 = 0\n",
        "# a2 = 0\n",
        "# a3 = 0\n",
        "# for i in range(9250):\n",
        "#   a = pynnd_m1.neighbours[0][0.15][i]\n",
        "#   b = pynnd_m1.neighbours[1][0.15][i]\n",
        "#   c = pynnd_m1.neighbours[2][0.15][i]\n",
        "#   d = pynnd_m1.neighbours[3][0.15][i]\n",
        "\n",
        "#   a1 += np.sum(a != b)\n",
        "#   a2 += np.sum(b != c)\n",
        "#   a3 +=np.sum(c != d)\n",
        "\n",
        "# print(a1/9250)\n",
        "# print(a2/9250)\n",
        "# print(a3/9250)"
      ],
      "metadata": {
        "id": "BqsSAyYdRQNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 1 - 3 layers"
      ],
      "metadata": {
        "id": "Vrp58uhu3jwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "neighbours_layers_indexes = [0,1,2]\n",
        "knn_layers_count = len(neighbours_layers_indexes)\n",
        "\n",
        "model1adv = Adversaries(\n",
        "    model = model1,\n",
        "    epsilons = [0.15],\n",
        "    folder = 'model1_three_lay/')\n",
        "\n",
        "activations_ttc = datasets_activations(model1, model1adv, layers_activations_small, neighbours_layers_indexes)\n",
        "\n",
        "pynnd_m1 = PyNND(\n",
        "    model1,\n",
        "    activations_ttc,\n",
        "    neighbours_layers_indexes,\n",
        "    model1adv.epsilons,\n",
        "    'model1_three_lay/'\n",
        "    )\n",
        "dknn_pynnd_m1 = DkNN(pynnd_m1)"
      ],
      "metadata": {
        "id": "YiK3MfgADao9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neighbours_layers_indexes = [0,1,2]\n",
        "knn_layers_count = len(neighbours_layers_indexes)\n",
        "\n",
        "if False:\n",
        "  model1adv = Adversaries(\n",
        "      model = model1,\n",
        "      epsilons = [0.15],\n",
        "      folder = 'model1_three_lay/')\n",
        "\n",
        "  activations_ttc = datasets_activations(model1, model1adv, layers_activations_small, neighbours_layers_indexes)\n",
        "\n",
        "  test_falconn_m1 = False\n",
        "  test_pynnd_m1 = True\n",
        "  test_annoy_m1 = False\n",
        "\n",
        "  if test_falconn_m1:\n",
        "    falconn_m1 = Falconn(\n",
        "        model1,\n",
        "        activations_ttc,\n",
        "        neighbours_layers_indexes,\n",
        "        model1adv.epsilons,\n",
        "        'model1_three_lay/')\n",
        "    dknn_falconn_m1 = DkNN(falconn_m1)\n",
        "\n",
        "  if test_pynnd_m1:\n",
        "    pynnd_m1 = PyNND(\n",
        "        model1,\n",
        "        activations_ttc,\n",
        "        neighbours_layers_indexes,\n",
        "        model1adv.epsilons,\n",
        "        'model1_three_lay/'\n",
        "        )\n",
        "    dknn_pynnd_m1 = DkNN(pynnd_m1)\n",
        "\n",
        "  if test_annoy_m1:\n",
        "    annoy_m1 = Annoy(\n",
        "        model1,\n",
        "        activations_ttc,\n",
        "        neighbours_layers_indexes,\n",
        "        model1adv.epsilons,\n",
        "        'model1_three_lay/')\n",
        "\n",
        "    dknn_annoy_m1 = DkNN(annoy_m1)"
      ],
      "metadata": {
        "id": "v4nLclCcmTKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Test model 2**"
      ],
      "metadata": {
        "id": "noUd5gSm3p5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model2adv = Adversaries(\n",
        "    model = model2,\n",
        "    epsilons = [0.025, 0.05, 0.075, 0.1, 0.125, 0.15],\n",
        "    folder = 'model2/experiment2/')\n",
        "\n",
        "activations_ttc2 = datasets_activations(\n",
        "    model2, \n",
        "    model2adv, \n",
        "    layers_activations_big_data, \n",
        "    neighbours_layers_indexes2)"
      ],
      "metadata": {
        "id": "UjngWYFO3ibR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pynnd_m1 = PyNND(\n",
        "    model2,\n",
        "    activations_ttc2,\n",
        "    neighbours_layers_indexes2,\n",
        "    model2adv.epsilons,\n",
        "    'model2/experiment2/'\n",
        "    )\n",
        "\n",
        "dknn_pynnd_m1 = DkNN(pynnd_m1)"
      ],
      "metadata": {
        "id": "j7XHnYze4w5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compare Models"
      ],
      "metadata": {
        "id": "w8L65WdnayLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def test_model_accuracy(labels1, labels2):\n",
        "#   correct_labels = np.sum(labels1 == labels2)\n",
        "#   return correct_labels / labels1.shape[0] * 100\n",
        "\n",
        "# def accuracy_per_model(knn_predicted_labels):\n",
        "#   accuracies_plot_knn = []\n",
        "#   for eps in epsilons:\n",
        "#     model_accuracy = test_model_accuracy(knn_predicted_labels[eps], y_test_raw)\n",
        "#     accuracies_plot_knn.append(model_accuracy)\n",
        "#   return accuracies_plot_knn\n",
        "\n",
        "# accuracies_plot_knn = accuracy_per_model(knn_predicted_labels)\n",
        "# plot_accuracy_vs_epsilon(model, fgsm_adversarials)\n",
        "# plt.plot(epsilons, accuracies_plot_knn, linestyle='--', marker='^', color='r', label = 'DkNN - NND')\n",
        "# plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "OI1v1L31a260"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Faiss"
      ],
      "metadata": {
        "id": "7AdRK5UIM-Gf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# neighbours_index_faiss = {}\n",
        "# dim = activations_train[layer].shape[1]\n",
        "# ind = faiss.IndexLSH(dim, dim * 2)\n",
        "# for layer in range(knn_layers_count):\n",
        "#   neighbours_index_faiss[layer] = faiss.IndexLSH(activations_train[layer].shape[1], dim * 4)\n",
        "#   neighbours_index_faiss[layer].add(activations_train[layer])"
      ],
      "metadata": {
        "id": "xFG9hGqKU5BY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# neighbors_faiss = {}\n",
        "# for layer in range(knn_layers_count):\n",
        "#   neighbors_faiss[layer] = {}\n",
        "#   for eps in epsilons:\n",
        "#     neighbors_faiss[layer][eps] = neighbours_index_faiss[layer].search(activations[eps][layer], 75)[0]"
      ],
      "metadata": {
        "id": "zpVOj8kwVOmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "FvElOaiLaHQc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nmslib"
      ],
      "metadata": {
        "id": "f52vsA_LaIqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install nmslib\n",
        "# import nmslib"
      ],
      "metadata": {
        "id": "keIureYgbiAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# index_nmslib = nmslib.init(method='hnsw', space='cosinesimil')\n",
        "# index_nmslib.addDataPointBatch(activations_train[0])\n",
        "# index_nmslib.createIndex({'post': 2}, print_progress=True)\n",
        "# ids, distances = index_nmslib.knnQuery(activations[0.0][0], k=75)"
      ],
      "metadata": {
        "id": "O9g8mIa4cvO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Faiss 2"
      ],
      "metadata": {
        "id": "FBiM1oYmbpxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class IVPQIndex():\n",
        "#     def __init__(self, vectors, labels):\n",
        "#         self.dimension = vectors.shape[1]\n",
        "#         self.vectors = vectors.astype('float32')\n",
        "#         self.labels = labels    \n",
        "#         def build(self, number_of_partition=8, search_in_x_partitions=2, subvector_size=8):\n",
        "#           quantizer = faiss.IndexFlatL2(self.dimension)\n",
        "#           self.index = faiss.IndexIVFPQ(quantizer, \n",
        "#                                         self.dimension, \n",
        "#                                         number_of_partition, \n",
        "#                                         search_in_x_partitions, \n",
        "#                                         subvector_size)\n",
        "#           self.index.train(self.vectors)\n",
        "#           self.index.add(self.vectors)\n",
        "        \n",
        "#     def query(self, vectors, k=10):\n",
        "#         distances, indices = self.index.search(vectors, k) \n",
        "#         # I expect only query on one vector thus the slice\n",
        "#         return [self.labels[i] for i in indices[0]]"
      ],
      "metadata": {
        "id": "g1Vq944SaL4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# index_quan = faiss.IndexFlatL2(activations_train[layer].shape[1])"
      ],
      "metadata": {
        "id": "EtxLsJwEaNkg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "hbfHUacwXai7",
        "sauhy9NhkdfL",
        "HHM-XaoGbj4O",
        "7AdRK5UIM-Gf",
        "f52vsA_LaIqr",
        "FBiM1oYmbpxd",
        "zX-XkLDgVqvT"
      ],
      "machine_shape": "hm",
      "name": "base.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}